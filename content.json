{"meta":{"title":"curiousLei","subtitle":null,"description":"雷答在 Github 上的个人博客","author":"Lei da","url":"http://curiousLei.github.io"},"pages":[],"posts":[{"title":"用户评论情感极性判别","slug":"用户评论情感极性判别","date":"2018-07-09T10:34:52.000Z","updated":"2018-07-09T10:35:12.340Z","comments":true,"path":"2018/07/09/用户评论情感极性判别/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/用户评论情感极性判别/","excerpt":"本文章介绍百度点石平台上的一个训练赛的赛题代码，赛题是包括用户评论文字的情感判别的分类问题，赛题链接戳此处 数据预处理 使用测试数据和训练数据生成语料库 import numpy as npimport jiebaimport codecs# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_setdataAll=load_data(&apos;data_train.csv&apos;)dataTest=load_data(&apos;data_test.csv&apos;)csvfile = codecs.open(&quot;fenci_result.csv&quot;, &apos;w&apos;, &apos;utf-8&apos;)#f=open(&apos;fenci_result.txt&apos;,&apos;a&apos;)for item in dataAll: seg_list=jieba.cut(item[2])#使用结巴分词 csvfile.write(&quot; &quot;.join(seg_list))#以空格隔开把分好的词写入文件，形成语料#f.close()for item in dataTest: seg_list=jieba.cut(item[-1]) csvfile.write(&quot; &quot;.join(seg_list))","text":"本文章介绍百度点石平台上的一个训练赛的赛题代码，赛题是包括用户评论文字的情感判别的分类问题，赛题链接戳此处 数据预处理 使用测试数据和训练数据生成语料库 import numpy as npimport jiebaimport codecs# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_setdataAll=load_data(&apos;data_train.csv&apos;)dataTest=load_data(&apos;data_test.csv&apos;)csvfile = codecs.open(&quot;fenci_result.csv&quot;, &apos;w&apos;, &apos;utf-8&apos;)#f=open(&apos;fenci_result.txt&apos;,&apos;a&apos;)for item in dataAll: seg_list=jieba.cut(item[2])#使用结巴分词 csvfile.write(&quot; &quot;.join(seg_list))#以空格隔开把分好的词写入文件，形成语料#f.close()for item in dataTest: seg_list=jieba.cut(item[-1]) csvfile.write(&quot; &quot;.join(seg_list)) 利用语料库，使用word2vec工具，生成可备用的模型，用于将句子转化为向量 from gensim.models import word2vecimport logginglogging.basicConfig(format = &apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level = logging.INFO)sentences = word2vec.Text8Corpus(&quot;fenci_result.csv&quot;) # 加载语料model = word2vec.Word2Vec(sentences, size = 400) # 训练skip-gram模型# 保存模型，以便重用model.save(&quot;corpus.model&quot;)model.wv.save_word2vec_format(&quot;corpus.model.bin&quot;, binary = True) 数据训练与测试 感觉训练方式很简陋，有待改善 #本程序用来测试模型#coding=utf-8 import reimport numpy as npimport jiebafrom gensim.models import word2vecimport loggingimport codecsfrom sklearn.decomposition import PCAfrom sklearn.model_selection import train_test_splitfrom sklearn import svmfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score, precision_score, recall_score, \\ roc_curve # 导入指标库import prettytable # 导入表格库# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_set#写文件def write_result(array, outpuFilePath): with open(outpuFilePath, &apos;w&apos;) as output_file: for i in range(len(array)): output_file.write(&quot;%d,%d\\n&quot; % (i+1,array[i]))#将句子转化为向量def getWordVecs(wordList): vecs = [] for word in wordList: word = word.strip() try: vecs.append(model[word]) except KeyError: continue # vecs = np.concatenate(vecs) return np.array(vecs, dtype = &apos;float&apos;)model = word2vec.KeyedVectors.load_word2vec_format(&quot;corpus.model.bin&quot;, binary = True)# segList=jieba.cut(&apos;烤鸭还是不错的，别的菜没什么特殊的&apos;)# resultList = getWordVecs(segList)# print(sum(np.array(resultList))/2)dataAll=load_data(&apos;data_train.csv&apos;)X=[]y=[]dataAll=np.array(dataAll[:1500])for item in dataAll: #temp=int(item[-1]) #y.append(temp if temp!=0 else 1)#把0都替换成1，先对2和1进行分类 y.append(int(item[-1])) segList=jieba.cut(item[2]) vecList=getWordVecs(segList) if len(vecList) != 0: X.append(sum(np.array(vecList))/len(vecList))X=X[:]x_train=np.array(X)y_train=np.array(y)print(x_train)print(y_train)# x_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2], [2, 1], [3, 2]])# print(x_train)# 使用sklearn的PCA进行维度转换model_pca = PCA(n_components=0.95) # 建立PCA模型对象model_pca.fit(x_train) # 将数据集输入模型#model_pca.transform(x_train) # 对数据集进行转换映射newX=model_pca.fit_transform(x_train)#进行转换映射，并将转换后的赋给newXcomponents = model_pca.components_ # 获得转换后的所有主成分(不明白什么意思)components_var = model_pca.explained_variance_ # 获得各主成分的方差components_var_ratio = model_pca.explained_variance_ratio_ # 获得各主成分的方差占比print(&quot;\\n主成分分析：&quot;)print (components) # 打印输出前2个主成分print (len(components_var)) # 打印输出所有主成分的方差print (components_var_ratio) # 打印输出所有主成分的方差占比print(len(newX))print(len(newX[0]))X_train, X_test, y_train, y_test = train_test_split(newX, y_train, test_size=.3, random_state=0)clf = svm.SVC(C=1, kernel=&apos;linear&apos;,decision_function_shape=&apos;ovr&apos;)clf.fit(X_train, y_train)y_hat=clf.predict(X_test)##评价指标accuracy_s = accuracy_score(y_test, y_hat) # 准确率precision_s = precision_score(y_test, y_hat, average=&apos;macro&apos;) # 精确度recall_s = recall_score(y_test, y_hat, average=&apos;macro&apos;) # 召回率f1_s = f1_score(y_test, y_hat, average=&apos;weighted&apos;) # F1得分print(&apos;Accuracy:&apos;)print(accuracy_s)print(&apos;Precision:&apos;)print(precision_s)print(&apos;Recall:&apos;)print(recall_s)print(&apos;f-measure:&apos;)print(f1_s)##混淆矩阵confusion_m = confusion_matrix(y_test,y_hat) # 获得混淆矩阵confusion_matrix_table = prettytable.PrettyTable() # 创建表格实例confusion_matrix_table.add_row(confusion_m[0, :]) # 增加第一行数据confusion_matrix_table.add_row(confusion_m[1, :]) # 增加第二行数据confusion_matrix_table.add_row(confusion_m[2, :]) # 增加第三行数据print (&apos;confusion matrix&apos;)print (confusion_matrix_table) # 打印输出混淆矩阵write_result(y_hat,&apos;print.csv&apos;) 预测阶段 使用所有训练数据训练模型并对test数据进行预测 #本程序用来进行预测#coding=utf-8 import reimport numpy as npimport jiebafrom gensim.models import word2vecimport loggingimport codecsfrom sklearn.decomposition import PCAfrom sklearn.model_selection import train_test_splitfrom sklearn import svm# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&apos;\\t&apos;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_set#写文件def write_result(array, outpuFilePath): with open(outpuFilePath, &apos;w&apos;) as output_file: for i in range(len(array)): output_file.write(&quot;%d,%d\\n&quot; % (i+1,array[i]))#将句子转化为向量def getWordVecs(wordList): vecs = [] for word in wordList: word = word.strip() try: vecs.append(model[word]) except KeyError: continue # vecs = np.concatenate(vecs) return np.array(vecs, dtype = &apos;float&apos;)#对预测数据进行处理def preDataHandle(): preData=load_data(&apos;data_test.csv&apos;) #exit(0) xPre=[] i=0 k=0 for item in preData: i+=1 s=&apos;&apos; for j in range(len(item)): if(j&gt;1): s=&quot;%s%s&quot;%(s,item[j]) segList=jieba.cut(s) vecList=getWordVecs(segList) if len(vecList) != 0: xPre.append(sum(np.array(vecList))/len(vecList)) else: k+=1 print(&apos;存在vecList长度为0的情况&apos;) print(item) x_pre=np.array(xPre) model_pca = PCA(n_components=factorNum) # 建立PCA模型对象 model_pca.fit(x_pre) # 将数据集输入模型 x_pre=model_pca.fit_transform(x_pre)#进行转换映射 return x_premodel = word2vec.KeyedVectors.load_word2vec_format(&quot;corpus.model.bin&quot;, binary = True)dataAll=load_data(&apos;data_train.csv&apos;)X=[]y=[]#dataAll=np.array(dataAll[:1500])for item in dataAll: print(item) y.append(int(item[-1])) segList=jieba.cut(item[2]) vecList=getWordVecs(segList) if len(vecList) != 0: X.append(sum(np.array(vecList))/len(vecList)) else: print(item)X=X[:]x_train=np.array(X)y_train=np.array(y)model_pca = PCA(n_components=0.95) # 建立PCA模型对象model_pca.fit(x_train) # 将数据集输入模型#model_pca.transform(x_train) # 对数据集进行转换映射newX=model_pca.fit_transform(x_train)#进行转换映射，并将转换后的赋给newXfactorNum=len(newX[0])clf = svm.SVC(C=1, kernel=&apos;linear&apos;,decision_function_shape=&apos;ovr&apos;)clf.fit(newX, y_train)x_pre=preDataHandle()y_pre=clf.predict(x_pre)write_result(y_pre,&apos;output.csv&apos;)print(&apos;Project has been finished successfully!&apos;) 比赛平台上计算出的结果f1-score为0.7249，很低，希望再接再厉","categories":[],"tags":[]},{"title":"Hello,Hexo!","slug":"Hello-Hexo","date":"2018-07-09T06:58:15.000Z","updated":"2018-07-09T08:35:20.314Z","comments":true,"path":"2018/07/09/Hello-Hexo/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/Hello-Hexo/","excerpt":"常见命令hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面（新建了一个md文件组,但目前不知道怎么用）hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo clean #清理public的内容","text":"常见命令hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面（新建了一个md文件组,但目前不知道怎么用）hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo clean #清理public的内容 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令hexo s -g #生成并本地预览hexo d -g #生成并上传","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-07-09T06:28:05.440Z","updated":"2018-07-09T06:28:05.440Z","comments":true,"path":"2018/07/09/hello-world/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}