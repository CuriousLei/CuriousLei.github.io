{"meta":{"title":"curiousLei","subtitle":null,"description":"雷答在 Github 上的个人博客","author":"Lei da","url":"http://curiousLei.github.io"},"pages":[],"posts":[{"title":"https原理总结及搭建(自签署证书)","slug":"https原理总结及搭建(自签署证书)","date":"2019-01-06T09:43:12.851Z","updated":"2019-01-06T09:48:10.398Z","comments":true,"path":"2019/01/06/https原理总结及搭建(自签署证书)/","link":"","permalink":"http://curiousLei.github.io/2019/01/06/https原理总结及搭建(自签署证书)/","excerpt":"","text":"最近在公司项目的服务器上做一些内部接口，要求使用https，于是花时间研究了一波。我们熟知的http在传输时未对数据进行加密，在传输一些敏感信息时存在着不小的安全隐患。因此，https在http的基础上加上了SSL（Secure Sockets Layer）加密，以保障数据的安全传输。如今使用的TLS实际上是SSL的升级版本。具体有关https的概念可参考百科https介绍 1.https原理探究https的保障信息安全的机制，其实用一句话就能概括：client与server通过非对称加密来协商一个对称秘钥，然后CS两端使用该对称秘钥来进行数据的加密解密，完成数据交互。所以数据传输时，实际上走的是对称加密。当然理解这句话前提，需要明白对称和非对称加密的原理，本文不做讨论。 原理概况 https交互原理图 大致过程： client发送请求 server返回证书 client验证并取出证书中的公钥 client生成随机数，并使用服务器公钥将其加密，把得到的密文发送给server server使用私钥解密，得到随机数 两端各自通过随机数生成对称秘钥，协商完成 数字证书与数字签名在详细介绍握手环节之前，我想先说说数字数字证书的起因及原理，数字证书是整个SSL加密的核心与纽带。首先，在使用非对称加密传输之前，客户端需要获取服务器公钥，这里存在一种攻击方式，即中间方使用自己的公钥替换服务器的公钥发送给客户端，再通过自己的私钥获取客户端传来的非对称加密内容，从而实现篡改以及窃听。为了方便理解，网上有一张图我直接拿来用了，如下所示。 为了防止获取公钥过程遭到第三方的掉包等之类的破坏，于是便有了证书机制，下图为服务器证书的签署以及验证的大致流程。 证书包含三部分内容 证书内容（服务器公钥、服务器信息等） 加密算法（加密算法、哈希算法） 密文（使用哈希算法计算证书内容得到哈希摘要，再使用CA私钥加密该摘要即得到密文，该过程称为数字签名） 验证数字证书 客户端验证服务器证书时，需要获取到你的上一级CA证书，从而得到取CA公钥，使用CA公钥对证书中的密文解密得到哈希摘要，同时客户端使用同样的哈希算法对服务器证书内容计算得到另一个哈希摘要，若这两个摘要相等，则证明证书合法。 当然，如何证明上级CA证书合法，这就要涉及到一个证书信任链的问题。上级证书通过更高一级的CA根证书来确定其合法性，这是一个递归向上的过程，直到最顶层根证书。顶层CA根证书是整个安全体系的根本。 上述的哈希签名也称为数字指纹法，该方法的精髓在于，相同的明文通过哈希计算得到的摘要，一定是相同的，而只要两份明文只要有一丝丝区别，其对应的哈希值也是不同的。因此，若第三方替换了证书中的公钥，根据证书内容计算出的新的摘要一定与密文中的摘要有所差异的，故可以轻松地判断证书不合法。 这里我提出了一个疑问，只替换公钥显然不行，那如果第三方把整个证书都替换成自己的证书（因为CA机构可以给任何人签名，黑客也可以），这样的话客户端的验证是不是可以通过？ 答案当然是否定的，很简单，因为证书内容里的服务器信息是唯一的、不可复制的，例如域名，若替换整个证书，域名也会变成黑客自己的域名，浏览器不会接受域名和请求内容不匹配的证书。比如说，浏览器请求了 baidu.com，结果返回了个google.com的证书，毫无疑问会立即排除掉。 保证了服务器公钥安全抵达客户端手中，后续的对话秘钥的协商便也能顺理成章地进行。因此https所采用的SSL机制是绝对安全的，几乎没有人能够破解。当然，有得必有失，https花费的开销也远高于http。 SSL握手过程理解了上述的证书机制，其实SSL加密机制也基本容易理解了，下面细究一下SSL握手过程，此处结合上方交互原理图进行分析（1） Client Helllo。客户端发送初次请求，请求内容包含版本信息，加密套件候选列表，压缩算法候选列表，随机数random_1，扩展字段等信息，以明文传输； （2）服务器选择客户端支持的加密套件、压缩算法、协议版本等，生成随机数random_2； （3）服务器将上述算法以及随机数等发送给客户端； （4）服务器发送服务器数字证书； （5）客户端接收服务器选择的算法以及随机数等，验证数字证书。若证书验证通过，或者用户接受了不可信证书，客户端获取服务器公钥，同时会生成随机数random_3，并使用服务器公钥加密该随机数得到密文； （6）客户端将第五步得到的密文传给服务器，由于公钥加密的内容只能使用私钥解开，所以random_3无法被窃听； （7）Change cipher Spec。客户端通知服务器协商完成； 此时客户端已存有三个随机数random_1、random_2和random_3，前两个是可以被截获的，第三个是私密的，根据这三者可计算得出对话秘钥，即enc_key=Fuc(random_1, random_2, random_3) （8）客户端结合之前所有通信参数的 hash 值与其它相关信息生成一段数据，并使用对话秘钥enc_key和算法将其加密，得到密文encrypted_handshake_message，将其发送给服务器进行验证； （9）服务器使用私钥解密第六步得到的密文，得到随机数random_3，此时服务器也拥有了三个随机数random_1、random_2和random_3，同样可计算出对话秘钥enc_key，至此双方共享对称加密秘钥的目的已达成；计算之前所有接收信息的 hash 值，然后解密客户端发送的 encrypted_handshake_message，验证数据和密钥正确性; （10）类似7和8，服务器通知客户端协商完成，同时计算发送encrypted_handshake_message。客户端以同样的方式验证encrypted_handshake_message，握手完成。 完成握手之后，服务器和客户端都使用相同的对话秘钥enc_key，对消息内容进行加密，实现安全通信。","categories":[],"tags":[]},{"title":"用户评论情感极性判别","slug":"用户评论情感极性判别","date":"2018-07-09T10:34:52.000Z","updated":"2018-07-09T10:35:12.340Z","comments":true,"path":"2018/07/09/用户评论情感极性判别/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/用户评论情感极性判别/","excerpt":"本文章介绍百度点石平台上的一个训练赛的赛题代码，赛题是包括用户评论文字的情感判别的分类问题，赛题链接戳此处 数据预处理 使用测试数据和训练数据生成语料库 import numpy as npimport jiebaimport codecs# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_setdataAll=load_data(&apos;data_train.csv&apos;)dataTest=load_data(&apos;data_test.csv&apos;)csvfile = codecs.open(&quot;fenci_result.csv&quot;, &apos;w&apos;, &apos;utf-8&apos;)#f=open(&apos;fenci_result.txt&apos;,&apos;a&apos;)for item in dataAll: seg_list=jieba.cut(item[2])#使用结巴分词 csvfile.write(&quot; &quot;.join(seg_list))#以空格隔开把分好的词写入文件，形成语料#f.close()for item in dataTest: seg_list=jieba.cut(item[-1]) csvfile.write(&quot; &quot;.join(seg_list))","text":"本文章介绍百度点石平台上的一个训练赛的赛题代码，赛题是包括用户评论文字的情感判别的分类问题，赛题链接戳此处 数据预处理 使用测试数据和训练数据生成语料库 import numpy as npimport jiebaimport codecs# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_setdataAll=load_data(&apos;data_train.csv&apos;)dataTest=load_data(&apos;data_test.csv&apos;)csvfile = codecs.open(&quot;fenci_result.csv&quot;, &apos;w&apos;, &apos;utf-8&apos;)#f=open(&apos;fenci_result.txt&apos;,&apos;a&apos;)for item in dataAll: seg_list=jieba.cut(item[2])#使用结巴分词 csvfile.write(&quot; &quot;.join(seg_list))#以空格隔开把分好的词写入文件，形成语料#f.close()for item in dataTest: seg_list=jieba.cut(item[-1]) csvfile.write(&quot; &quot;.join(seg_list)) 利用语料库，使用word2vec工具，生成可备用的模型，用于将句子转化为向量 from gensim.models import word2vecimport logginglogging.basicConfig(format = &apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level = logging.INFO)sentences = word2vec.Text8Corpus(&quot;fenci_result.csv&quot;) # 加载语料model = word2vec.Word2Vec(sentences, size = 400) # 训练skip-gram模型# 保存模型，以便重用model.save(&quot;corpus.model&quot;)model.wv.save_word2vec_format(&quot;corpus.model.bin&quot;, binary = True) 数据训练与测试 感觉训练方式很简陋，有待改善 #本程序用来测试模型#coding=utf-8 import reimport numpy as npimport jiebafrom gensim.models import word2vecimport loggingimport codecsfrom sklearn.decomposition import PCAfrom sklearn.model_selection import train_test_splitfrom sklearn import svmfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score, precision_score, recall_score, \\ roc_curve # 导入指标库import prettytable # 导入表格库# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&quot;\\t&quot;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_set#写文件def write_result(array, outpuFilePath): with open(outpuFilePath, &apos;w&apos;) as output_file: for i in range(len(array)): output_file.write(&quot;%d,%d\\n&quot; % (i+1,array[i]))#将句子转化为向量def getWordVecs(wordList): vecs = [] for word in wordList: word = word.strip() try: vecs.append(model[word]) except KeyError: continue # vecs = np.concatenate(vecs) return np.array(vecs, dtype = &apos;float&apos;)model = word2vec.KeyedVectors.load_word2vec_format(&quot;corpus.model.bin&quot;, binary = True)# segList=jieba.cut(&apos;烤鸭还是不错的，别的菜没什么特殊的&apos;)# resultList = getWordVecs(segList)# print(sum(np.array(resultList))/2)dataAll=load_data(&apos;data_train.csv&apos;)X=[]y=[]dataAll=np.array(dataAll[:1500])for item in dataAll: #temp=int(item[-1]) #y.append(temp if temp!=0 else 1)#把0都替换成1，先对2和1进行分类 y.append(int(item[-1])) segList=jieba.cut(item[2]) vecList=getWordVecs(segList) if len(vecList) != 0: X.append(sum(np.array(vecList))/len(vecList))X=X[:]x_train=np.array(X)y_train=np.array(y)print(x_train)print(y_train)# x_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2], [2, 1], [3, 2]])# print(x_train)# 使用sklearn的PCA进行维度转换model_pca = PCA(n_components=0.95) # 建立PCA模型对象model_pca.fit(x_train) # 将数据集输入模型#model_pca.transform(x_train) # 对数据集进行转换映射newX=model_pca.fit_transform(x_train)#进行转换映射，并将转换后的赋给newXcomponents = model_pca.components_ # 获得转换后的所有主成分(不明白什么意思)components_var = model_pca.explained_variance_ # 获得各主成分的方差components_var_ratio = model_pca.explained_variance_ratio_ # 获得各主成分的方差占比print(&quot;\\n主成分分析：&quot;)print (components) # 打印输出前2个主成分print (len(components_var)) # 打印输出所有主成分的方差print (components_var_ratio) # 打印输出所有主成分的方差占比print(len(newX))print(len(newX[0]))X_train, X_test, y_train, y_test = train_test_split(newX, y_train, test_size=.3, random_state=0)clf = svm.SVC(C=1, kernel=&apos;linear&apos;,decision_function_shape=&apos;ovr&apos;)clf.fit(X_train, y_train)y_hat=clf.predict(X_test)##评价指标accuracy_s = accuracy_score(y_test, y_hat) # 准确率precision_s = precision_score(y_test, y_hat, average=&apos;macro&apos;) # 精确度recall_s = recall_score(y_test, y_hat, average=&apos;macro&apos;) # 召回率f1_s = f1_score(y_test, y_hat, average=&apos;weighted&apos;) # F1得分print(&apos;Accuracy:&apos;)print(accuracy_s)print(&apos;Precision:&apos;)print(precision_s)print(&apos;Recall:&apos;)print(recall_s)print(&apos;f-measure:&apos;)print(f1_s)##混淆矩阵confusion_m = confusion_matrix(y_test,y_hat) # 获得混淆矩阵confusion_matrix_table = prettytable.PrettyTable() # 创建表格实例confusion_matrix_table.add_row(confusion_m[0, :]) # 增加第一行数据confusion_matrix_table.add_row(confusion_m[1, :]) # 增加第二行数据confusion_matrix_table.add_row(confusion_m[2, :]) # 增加第三行数据print (&apos;confusion matrix&apos;)print (confusion_matrix_table) # 打印输出混淆矩阵write_result(y_hat,&apos;print.csv&apos;) 预测阶段 使用所有训练数据训练模型并对test数据进行预测 #本程序用来进行预测#coding=utf-8 import reimport numpy as npimport jiebafrom gensim.models import word2vecimport loggingimport codecsfrom sklearn.decomposition import PCAfrom sklearn.model_selection import train_test_splitfrom sklearn import svm# 该函数作用是读取文件def load_data(file_path): data_set = [] with open(file_path, &apos;r&apos;) as lines: for line in lines: line=line.strip() values=line.split(&apos;\\t&apos;) data_set.append(values) np.array(data_set) # print(data_set[0]) return data_set#写文件def write_result(array, outpuFilePath): with open(outpuFilePath, &apos;w&apos;) as output_file: for i in range(len(array)): output_file.write(&quot;%d,%d\\n&quot; % (i+1,array[i]))#将句子转化为向量def getWordVecs(wordList): vecs = [] for word in wordList: word = word.strip() try: vecs.append(model[word]) except KeyError: continue # vecs = np.concatenate(vecs) return np.array(vecs, dtype = &apos;float&apos;)#对预测数据进行处理def preDataHandle(): preData=load_data(&apos;data_test.csv&apos;) #exit(0) xPre=[] i=0 k=0 for item in preData: i+=1 s=&apos;&apos; for j in range(len(item)): if(j&gt;1): s=&quot;%s%s&quot;%(s,item[j]) segList=jieba.cut(s) vecList=getWordVecs(segList) if len(vecList) != 0: xPre.append(sum(np.array(vecList))/len(vecList)) else: k+=1 print(&apos;存在vecList长度为0的情况&apos;) print(item) x_pre=np.array(xPre) model_pca = PCA(n_components=factorNum) # 建立PCA模型对象 model_pca.fit(x_pre) # 将数据集输入模型 x_pre=model_pca.fit_transform(x_pre)#进行转换映射 return x_premodel = word2vec.KeyedVectors.load_word2vec_format(&quot;corpus.model.bin&quot;, binary = True)dataAll=load_data(&apos;data_train.csv&apos;)X=[]y=[]#dataAll=np.array(dataAll[:1500])for item in dataAll: print(item) y.append(int(item[-1])) segList=jieba.cut(item[2]) vecList=getWordVecs(segList) if len(vecList) != 0: X.append(sum(np.array(vecList))/len(vecList)) else: print(item)X=X[:]x_train=np.array(X)y_train=np.array(y)model_pca = PCA(n_components=0.95) # 建立PCA模型对象model_pca.fit(x_train) # 将数据集输入模型#model_pca.transform(x_train) # 对数据集进行转换映射newX=model_pca.fit_transform(x_train)#进行转换映射，并将转换后的赋给newXfactorNum=len(newX[0])clf = svm.SVC(C=1, kernel=&apos;linear&apos;,decision_function_shape=&apos;ovr&apos;)clf.fit(newX, y_train)x_pre=preDataHandle()y_pre=clf.predict(x_pre)write_result(y_pre,&apos;output.csv&apos;)print(&apos;Project has been finished successfully!&apos;) 比赛平台上计算出的结果f1-score为0.7249，很低，希望再接再厉","categories":[],"tags":[]},{"title":"Hello,Hexo!","slug":"Hello-Hexo","date":"2018-07-09T06:58:15.000Z","updated":"2018-07-09T08:35:20.314Z","comments":true,"path":"2018/07/09/Hello-Hexo/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/Hello-Hexo/","excerpt":"常见命令hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面（新建了一个md文件组,但目前不知道怎么用）hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo clean #清理public的内容","text":"常见命令hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面（新建了一个md文件组,但目前不知道怎么用）hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo clean #清理public的内容 缩写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令hexo s -g #生成并本地预览hexo d -g #生成并上传","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-07-09T06:28:05.440Z","updated":"2018-07-09T06:28:05.440Z","comments":true,"path":"2018/07/09/hello-world/","link":"","permalink":"http://curiousLei.github.io/2018/07/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}